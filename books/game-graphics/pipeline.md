---
title: "レンダリングパイプライン"
free: true
---

## グラフィックスライブラリ

### セマンティックギャップとアルゴリズム

パソコンはいろんな仕事ができる便利な道具ですが、今のところユーザが期待していることを自動的にやってくれるほど賢い道具にはなっていません。ユーザは目的の仕事を達成するために、パソコンに用意されている機能を使った仕事の手順を考える必要があります。このように、ユーザが「やりたいこと」と、コンピュータが「できること」の間には、意味的なギャップが存在します。このようなギャップはセマンティックギャップと呼ばれます。

コンピュータのアプリケーションプログラムとコンピュータのハードウェアとの間にも、同じようなセマンティックギャップが存在します (図 2)。例えば、グラフィックス系のアプリケーションプログラムにおいて、画面上に線を引くニーズがあったとします。これに対して、コンピュータのハードウェアが画面上に点を打つ機能しか持っていなければ、そのままでは線を引くことはできません。そこで、点を打つ機能を組み合わせて線分を描く必要があります (図 3)。この「点を組み合わせて線分を描く方法」がアルゴリズムです (Bresenham のアルゴリズムなど)。

当初、このアルゴリズムは、すべてアプリケーションプログラム内に組み込まれていました。しかし、ひとつのコンピュータ上で複数のアプリケーションプログラムを使用すると、この方法では、同じ目的のプログラムコードが、アプリケーションごとに重複して存在してしまいます。これはメモリなどのコンピュータのリソース上も無駄ですし、ソフトウェア開発の面でも、同じ目的の異なるプログラムコードを作成することになって、コストの上昇を招きます。

そこで、アプリケーションプログラムがグラフィックスハードウェアに要求する機能を整理して、それをアプリケーションプログラムから分離するということが行われました。このアプリケーションプログラムが要求する機能を実現するグラフィックスアルゴリズムを集めたソフトウェアの層が、グラフィックスライブラリです (図 4)。

<figure>
<img src="media/image1.emf" style="width:6.05694in;height:3.80694in" />
<figcaption><p>図 2 セマンティックギャップとアルゴリズム</p></figcaption>
</figure>

<figure>
<img src="media/image2.emf" style="width:6.26389in;height:3.01875in" />
<figcaption><p>図 3 アルゴリズムの役割</p></figcaption>
</figure>

なお、前述の「線分を描く方法」のような簡単なアルゴリズムは、実際にはほとんどのグラフィックスハードウェアに固定機能として実装されています。またハードウェア技術の向上により、近年はより高度な機能がグラフィックスハードウェアに実装されています。すなわち、グラフィックスハードウェア自体にもアルゴリズムが組み込まれているのです。

<figure>
<img src="media/image3.emf" style="width:6.05694in;height:3.80694in" />
<figcaption><p>図 4 アルゴリズムの実装場所</p></figcaption>
</figure>

### グラフィックスライブラリの二つの役割

このようにグラフィックスライブラリは、アプリケーションプログラムとグラフィックスハードウェアの仲立ちとして、アプリケーションプログラムにグラフィックスの機能を提供します。この役割には、次の二つがあります。

#### 標準的なグラフィックスアルゴリズムを提供する

グラフィックスの機能をアプリケーションプログラムから分離することによって、次のようなメリットが生まれます。

- ソフトウェア開発の手間を減じる

- ソフトウェアのポータビリティを向上する

このようなグラフィックスライブラリは、ACM CORE、ISO GKS、GKS-3D、PHIGS、PHIGS+ など数多くのものが提案されてきており、また標準化も試みられました。しかし、現在ではこのような形式のグラフィックスライブラリは、あまり使われなくなりました。これは技術の進歩により要求されるグラフィックスの機能が多様化し、単一のグラフィックスライブラリの層ではそれに対応しきれなかったことも、理由のひとつだと思います。

#### グラフィックスハードウェアの機能を呼び出す方法を提供する

グラフィックスハードウェアの高機能化にともなって、グラフィックスライブラリはその複雑な機能を抽象化してアプリケーションプログラムに提供する、インタフェースの役割 (API) を担うようになりました。

- グラフィックスハードウェアの機能を抽象化する

- アプリケーションとのインタフェース (API) を提供する

現在の代表的なグラフィックスライブラリである OpenGL や DirectX は、そのような性格を持つものです。これらも複雑なアルゴリズムをソフトウェアで実装した、高度な機能を提供していますが、今後そのような機能は別の層 (ミドルウェア等) に移管され、グラフィックスライブラリ自体はハードウェアの機能の抽象化に特化する方向にあると考えられます (図 5)。

### GPU におけるグラフィックスライブラリ

プログラマブルなグラフィックスハードウェアとなった現在の GPU では、アプリケーションプログラムからグラフィックスハードウェアのプログラマブルシェーダ上にプログラムを導入して、必要な機能を実現します。このプログラムをシェーダプログラムといいます。この場合、グラフィックスの機能は、CPU 側のプログラムとシェーダプログラム、そしてグラフィックスハードウェアの固定機能の組み合わせで実現されます。

このため、このグライックスライブラリは、アプリケーションプログラムに対してグラフィックスの機能を提供するだけでなく、シェーダプログラムや GPU 上のメモリ、演算機能などのリソースを管理する、OS のような役割を果たします。

<figure>
<img src="media/image4.emf" style="width:6.05694in;height:3.80694in" />
<figcaption><p>図 5 GPU におけるグラフィックスライブラリ</p></figcaption>
</figure>

### ミドルウェア

グラフィックスライブラリがグラフィックスハードウェアの機能の呼び出しに特化する一方で、より高度な機能は、ミドルウェアと呼ばれる上位の層に移管されるようなりました。これを含めてグラフィックスライブラリとしてとらえることもできます。この層はアプリケーションプログラムの要求に合わせて、ミドルウェアごとに異なる機能を提供します (図 6)。

たとえば、CG のシーンを構成するデータ (オブジェクト) の階層的な管理に用いられるシーングラフの操作とその映像生成を行うシーングラフ API もそのひとつで、初期の OpenInventor をはじめ、OpenSceneGraph や SceniX などがあります。また、ゲームに使われる機能を集めたものはゲームエンジンと呼ばれ、CryENGINE、Unreal Engine、Unity、OROCHI、MascotCapsule、Irrlicht などの総合的なもの、chidori、DAIKOKU、OGRE などの映像生成主体のもの、BISHAMON、YEBIS などのエフェクト生成用のもの、CRI-ADX2 のようなサウンド用のもの、それに衝突などの物理シミュレーションを行う PhysX や Havok、ODE、Bullet などの物理エンジンなど、目的に応じて様々なものがあります。

現在のゲーム開発に使われている各種の技術は非常に高度になってきているため、個々のゲームメーカーが独自に対応することが困難になりつつあります。このようなミドルウェアの特徴や特性を知り、うまく組み合わせて使うことも、現在のゲーム開発では重要になっています。

<figure>
<img src="media/image5.emf" style="width:6.05694in;height:4.43194in" />
<figcaption><p>図 6 多様なアプリケーションとミドルウェア</p></figcaption>
</figure>

<figure>
<img src="media/image6.emf" style="width:6.05694in;height:3.68194in" />
<figcaption><p>図 7 レンダリングのイメージ</p></figcaption>
</figure>

## レンダリング

### 3DCG におけるレンダリング

デザイン用語としてのレンダリングはデザイナが自分のイメージを可視化することをいいますが、3DCG ではコンピュータによってシーンのデータから映像を生成することを示します。シーンのデータは、配置する物体の形状情報や、その表面の材質情報、それにシーン中の光源に関する情報、視点あるいはカメラの位置や方向などの視点情報などから構成されます。これらをもとに、コンピュータは視点位置から見たシーンの映像を生成します (図 7)。

### レンダリングの二つの方向

現在用いられているレンダリング手法は、サンプリングによる方法とラスタライズによる方法の二つに大別できます。

#### サンプリングによる方法

これは視点を出発してスクリーン上の一点を通る半直線、すなわち視線と物体との交点を求め、交点における陰影を求めてスクリーン上の点の色とする方法です (図 8)。レイキャスティング法やレイトレーシング法がこれに分類されます。この方法は計算のモデルが単純で複雑な光学現象の再現が行いやすいため、高品質な映像生成を行う場合によく用いられます。しかし、一般に計算量が多く、高速な映像生成には向かないと考えられています。

<figure>
<img src="media/image7.png" style="width:3.06736in;height:1.71042in" alt="fig2" />
<figcaption><p>図 8 サンプリングによるレンダリング</p></figcaption>
</figure>

#### ラスタライズによる方法

これは物体のスクリーン上への投影像を求め、その投影像の領域をラスタライズにより塗り潰す方法です。スキャンライン法やデプスバッファ法がこれに分類されます。この方法により描画される形状は、主にスクリーンへの投影や塗りつぶしが簡単な多面体になります。また、複雑な光学現象の再現には向かないため、生成される映像はリアルさに欠ける場合があります。しかしハードウェアによる実装が行われており、高速な映像生成が目的の場合には、一般的にこの方法が用いられます (図 9)。

<figure>
<img src="media/image8.png" style="width:3.06736in;height:1.71042in" alt="fig1" />
<figcaption><p>図 9 ラスタライズによるレンダリング</p></figcaption>
</figure>

映画のように高品質な映像の生成には主としてサンプリングによる方法が用いられており、これは現在でも時間がかかります。処理時間を短縮するために計算のモデルなどを単純化すれば、得られる映像の品質が損なわれます。処理時間と品質はトレードオフの関係にあります。

しかし現在では、これら二つの方法は互いに近づきつつあります。例えば、既にプログラマブルな GPU を用いてレイトレーシングをリアルタイムに実現する手法が提案されています。その一方で、デプスバッファ法により高品質な映像を生成する手法も数多く提案されています。

### リアルタイムレンダリング

リアルタイムレンダリングは、レンダリングしようとするシーンの変化に対して、遅れることなく映像を生成することをいいます。ゲームや CAD システムなどの場合、現在の画面表示に対するユーザの反応 (コントローラやマウスの操作など) によりシーンのデータが変更され、それが次の瞬間の画面表示に反映されます。このとき、この「反応」に対する「表示」のサイクルが十分高速であれば、ユーザはゲームに対して没入感を得ることができ、CAD システムに対して快適に対話的操作を行うことができるようになります。

コンピュータはディスプレイの画面への表示を連続的に行っているわけではなく、静止画を一定の時間間隔で書き換えています。この一枚の静止画をフレームといい、その一秒間あたりの描き替え回数をリフレッシュレートといいます。リフレッシュレートの単位には、周波数と同じ Hz (Hertz) が用いられます。この値はグラフィックスハードウェアとディスプレイとの関係によって決定される定数値です。これに対して、レンダリングによって一秒間に生成されるフレームの数をフレームレートといい、これには fps (frame per second) という単位が用いられます。リアルタイムレンダリングでは、一定以上のフレームレートを確保することが必要になります。

一般に動画として知覚するには、6 〜 8 fps 程度のフレームレートが必要だと言われます。しかし、これはかなりぎこちない表示になります。15 fps 以上あれば、表示はほぼ滑らかに変化するように感じられます。逆に、液晶などの一般的なフラットパネルディスプレイのリフレッシュレートは 60 Hz 程度のため、これを超えるフレームレートで映像を生成しても、すべてのフレームが表示されることはありません。したがって、リアルタイムレンダリングでは、通常は 60 fps のフレームレートを確保すれば十分です。

<figure>
<img src="media/image9.emf" style="width:6.05694in;height:1.31806in" />
<figcaption><p>表 1 フレームレートと対話性</p></figcaption>
</figure>

しかし、リフレッシュレートが 60 Hz のフラットパネルディスプレイでは、速い動きのときに残像が見えることがあります。これを避けるために、リフレッシュレートを120 Hz (倍速) や 240 Hz (四倍速) に高めたディスプレイも存在します。ゲームではコントローラの操作に対して画面の次の描き換えタイミングまで表示が遅れることを避けられないため、このようなディスプレイを用い、高いフレームレートで生成した映像の表示を行う場合もあります。

### グラフィックスハードウェアの重要性

リアルタイムレンダリングでは、通常、3次元のシーンを対象に映像を生成します。そのためには、単に画面上の領域を塗りつぶすだけでなく、3次元の図形をスクリーンに投影して画面上の領域を求め、その領域の色を照明計算により求める必要があります。リアルな陰影を得るには精密な物理現象の再現が必要になるため、長い計算時間を要します。また、コントローラの操作の受け付けや動き (アニメーション) の生成なども、これらの処理と並行して行わなければなりません。衝突の処理や変形、物理シミュレーションにもとづく破壊や粒体の表現なども、時間のかかる処理になります。コンピュータの CPU の性能が向上した現在でも、これらのすべてを CPU 単独で処理して十分な対話性を達成するのは困難です。

したがって現在のリアルタイムレンダリングでは、グラフィックスの処理を専門に行うグラフィックスハードウェア、すなわち GPU の支援が不可欠だと考えられています。既に GPU は現在の PC に必須のものとなっています。また、ほとんどの 3DCGアプリケーションは、GPU の利用を前提としています。

また、近年はプログラマブルな GPU の登場により、より多様なグラフィックスの処理が可能になりました。また、物理計算などのグラフィックス以外の処理にも GPU が用いられるようになり、映像だけでなく、動きのリアリズムの向上にも大きく貢献するようになりました。

## パイプライン

### 高速化手法としてのパイプライン処理

パイプラインは、もともと油田などから石油を運ぶ長い管のことをいいます。でもコンピュータでは、図 10のようにひとつの処理をいくつかの段階 (ステージ) に分割して、処理を順送りすることをいいます。これにより同じ時間がかかる処理でも、分割した各段階を同時に動作させることによって、全体的な処理量 (throughput) を増加させることができます。

<figure>
<img src="media/image10.emf" style="width:6.05694in;height:3.125in" />
<figcaption><p>図 10 パイプラインの考え方</p></figcaption>
</figure>

パイプライン処理は、各ステージの処理時間がすべて同じときに、最も高い性能が得られます。例えば、三段に分割すれば、三倍速くなります。しかし、もし一部のステージの処理時間が他のステージより長いと、そこで次の処理が待たされてしまい、処理を割り当てられないステージが発生してしまいます。これは全体の処理量を大きく低下させてしまいます。

<figure>
<img src="media/image11.emf" style="width:6.05694in;height:3.125in" />
<figcaption><p>図 11 パイプラインの考え方</p></figcaption>
</figure>

## グラフィックス処理のパイプライン

### グラフィックス処理の概念上のパイプライン

グラフィック処理の概念上のパイプラインは、アプリケーション、ジオメトリ処理、フラグメント処理の三つのステージで捉えることができます。

<figure>
<img src="media/image12.emf" style="width:5.99444in;height:1.36389in" />
<figcaption><p>図 12 グラフィックス処理の概念上のパイプライン</p></figcaption>
</figure>

### アプリケーションステージ

ユーザが実際に使用するアプリケーションソフトウェアのステージで、CPU 側で実行され、グラフィックスハードウェアにジオメトリデータ (形状情報) を送って画面表示の処理を依頼します。このステージはソフトウェアだけで構成されるため、開発者はすべてを制御することができます。また、ソフトウェアの実装を変更することで、動作を変更することができます。

ソフトウェアはこのステージで画面表示に必要なジオメトリデータを生成します。ジオメトリデータは点や線分、三角形といった基本図形 (Rendering Primitive) の種類と、その頂点の位置や色、法線ベクトルなどの頂点属性 (Attribute) で構成されます。これを次のステージであるジオメトリ処理に送ります。

アプリケーションステージでは、ユーザとの対話に用いるマウス、キーボード、ジョイスティック、位置センサ、イメージセンサなどのインタフェース機器からのデータの入力を管理します。表示している図形の衝突の検出などもこのステージで行われ、結果を画面表示に反映したり、フォース (力覚) フィードバック機能を持つインタフェース機器に返したりします。

また時間に関する処理、例えばアニメーションの生成におけるタイミングの制御なども、アプリケーションのステージでの仕事です。以前は時間に伴う変形などの処理もアプリケーションステージの仕事でしたが、現在の GPU は単純な座標変換によるアニメーションやスキニング (骨格に基づく変形)、ジオメトリモーフィング (個々の頂点の移動による変形) などは、ジオメトリ処理のステージで実行することが可能になっています。テクスチャアニメーションに関しても、あらかじめグラフィックスハードウェア側のメモリに転送しておいた複数のテクスチャを順次切り替える手法などを用いて実現することが可能になっています。

このほか、他の (グラフィックスハードウェア上の) ステージでは実現できないすべての種類の処理が、アプリケーションステージで実行されます。たとえば、グラフィクスハードウェアに送るジオメトリデータの量を減らす最適化処理などは、当然グラフィックスハードウェアでは実行できません。この処理には階層的な視錐台カリング (視野に含まれないものをあらかじめ除去する) やオクルージョンカリング (他の物体に隠されて見えないものをあらかじめ除去する) などがあります。なお、最も単純なカリングであるバックフェースカリング (視点に対して裏側を向いている背面ポリゴンの除去) はグラフィックスハードウェアに組み込まれています。

アプリケーションのステージにおける最適化処理の今後の課題は、現在主流となっているマルチコア CPU による並列処理に対応することでしょう。次のステージ、すなわちグラフィックスハードウェアのデータの入り口はひとつしかないため、並行して動作する複数の処理単位 (スレッド) から同時にデータを流し込むことができません。したがって、アプリケーションのステージの並列化は、現時点ではスレッドごとにカリング、衝突検出、物理シミュレーション、そしてジオメトリデータの送出などの異なる処理を実行することにより行われています。これはスレッドごとに処理内容が異なるために処理時間が揃いにくく、スレッド同士が互いに同期を取ることが困難になります。また、将来の CPU のメニィコア化への対応も今後の課題となっています。

### ジオメトリ処理ステージ

アプリケーションから受け取ったジオメトリデータ、すなわち基本図形と頂点属性を処理するステージで、座標変換や頂点の陰影付け、クリッピングなどを行います。

固定機能の (すなわち、プログラマブルではない) グラフィックスハードウェアでは、このステージは複数の固定機能のステージに分割して実装されていました (図 13)。また、さらに性能を向上するために、このパイプラインを複数並列に動作させることも行われました (図 14)。

これは、このステージにおいて座標変換や陰影付けなどの多くの実数演算を伴う負荷の高い処理が実行されるためです。しかし実数演算のハードウェアのコストは高いため、これは初期のパソコン用のグラフィックスハードウェアには搭載されていませんでした。そのために、このステージを CPU 側で実行する実装も行われていました。

その後、実数演算のハードウェアを搭載してジオメトリ処理を担当できるパソコン用のグラフィックスハードウェアが登場しました。これは GPU (Graphics Processing Unit) と名付けられました。また、ハードウェアによる座標変換や陰影付けの機能は、ハードウェア T & L (Transform and Lighting) と呼ばれました。

<figure>
<img src="media/image13.emf" style="width:5.86389in;height:2.23889in" />
<figcaption><p>図 13 ジオメトリ処理のパイプライン</p></figcaption>
</figure>

<figure>
<img src="media/image14.emf" style="width:6in;height:2.81806in" />
<figcaption><p>図 14 ジオメトリ処理のパイプラインの並列化</p></figcaption>
</figure>

#### モデル変換

シーンを構成するオブジェクト (部品図形) は、個々に独自の座標系で定義されています。これをローカル座標系といいます。シーンを構築するには、このオブジェクトを視点や光源なども配置される、ワールド座標系と呼ばれる単一の座標系に配置します。このために行われるローカル座標系からワールド座標系への座標変換をモデル変換といいます (図 15)。

モデル変換はオブジェクトごとに設定します。また、オブジェクト間に骨格のような階層構造があれば、この変換も階層的に合成して実行します。モデル変換後は、すべてのオブジェクトがワールド座標系上に存在します。

<figure>
<img src="media/image15.emf" style="width:6in;height:3.58542in" />
<figcaption><p>図 15 モデル変換</p></figcaption>
</figure>

#### ビュー変換

ワールド座標系で構築されたシーンの視点位置から見た映像を作成するために、視点を基準とする視点座標系にシーン全体を座標変換します。このために行われるワールド座標系から視点座標系への変換をビュー変換といいます (図 16)。なお、モデル変換とビュー変換は通常ひとつの座標変換に合成されます。この合成変換をモデルビュー変換といいます。ローカル座標系、ワールド座標系、視点座標系、およびスクリーンの関係を、図 17 に示します。

<figure>
<img src="media/image16.emf" style="width:6in;height:3.45486in" />
<figcaption><p>図 16 ビュー変換</p></figcaption>
</figure>

<figure>
<img src="media/image17.emf" style="width:5in;height:3.15347in" />
<figcaption><p>図 17 ローカル座標系、ワールド座標系、視点座標系の関係</p></figcaption>
</figure>

<figure>
<img src="media/image18.emf" style="width:6in;height:3.125in" />
<figcaption><p>図 18 陰影付けの有無</p></figcaption>
</figure>

#### 陰影付け

物体の表面に光を当てた時の反射光の強度を計算し、物体にリアルな外観を与えます (図 18)。ジオメトリ処理のステージでは、この処理は頂点単位に行われます。その結果の頂点データがラスタライズの際に補間され、画素 (ピクセル、Pixel) の陰影を求めるために用いられます。

実世界の光の反射は、光と物体表面との相互作用により発生しますが、リアルタイムレンダリングでは、この計算に多くの時間を割くことができません。このため、従来この計算には単純な関係式が用いられ、本物の反射 (映りこみ) や屈折、影 (シャドウ) などの再現は行われませんでした。しかし、近年は GPU によりかなり精密に計算することが可能になりました。

<figure>
<img src="media/image19.emf" style="width:5.94028in;height:3.49514in" />
<figcaption><p>図 19 ビューボリューム</p></figcaption>
</figure>

<figure>
<img src="media/image20.emf" style="width:6in;height:3.24722in" />
<figcaption><p>図 20 直交投影と透視投影</p></figcaption>
</figure>

#### 投影変換

視点座標系に配置されたシーンをスクリーンに投影する変換を投影変換といいます。ディスプレイの表示領域は有限ですから、そこに映るシーンの空間も限定されます。この空間をビューボリューム (View Volume) といいます。投影変換は、この空間を、中心が原点にあり一辺の長さが 2 の立方体の空間である標準ビューボリューム (Canonical View Volume) に変形します (図 19)。

このとき、変換前のビューボリュームに直方体を用いる場合と、四角錐台を用いる場合の二通りがあります。直方体を用いれば直交投影 (Orthogonal Projection) となり、四角錐台を用いれば透視投影 (Perspective Projection) となります (図 20)。なお、この四角錐台のビューボリュームは特にビューフラスタム (View Frustum, 視錐台) と呼ばれます。

<figure>
<img src="media/image21.emf" style="width:6.01111in;height:3.08542in" />
<figcaption><p>図 21 2次元のクリッピング</p></figcaption>
</figure>

<figure>
<img src="media/image22.emf" style="width:6in;height:3.09653in" />
<figcaption><p>図 22 ビューボリュームによるクリッピングのイメージ</p></figcaption>
</figure>

#### クリッピング

グラフィックスハードウェアは標準ビューボリュームからはみ出た図形を切り取り、この内部にある図形だけを画面に表示します。この処理をクリッピング (Clipping) といいます。このため、標準ビューボリュームはクリッピング空間とも呼ばれ、クリッピング空間の座標系はクリッピング座標系と呼ばれます。また、この座標系は次のビューポート変換によりデバイス座標系に対応づけられるので、正規化デバイス座標系 (Normalized Dvice Coordnate, NDC) と呼ばれます。

#### ビューポート変換

クリッピングの結果、ビューボリュームに収められたシーンの、ビューボリュームの xy 平面への直交投影像を、ディスプレイ上の表示領域にはめ込みます。このディスプレイ上の表示領域をビューポート (Viewport) といい、このはめ込みを行う変換をビューポート変換 (Viewport Transformation) あるいはスクリーンマッピング (Screen Mapping) と呼びます。

<figure>
<img src="media/image23.emf" style="width:6in;height:3.11389in" />
<figcaption><p>図 23 ビューポート変換</p></figcaption>
</figure>

<figure>
<img src="media/image24.emf" style="width:5.31806in;height:2.30694in" />
<figcaption><p>図 24 フラグメント処理のパイプライン</p></figcaption>
</figure>

### フラグメント処理ステージ

画像を構成する画素に関するデータ、いわゆるフラグメントデータを処理するステージです。まず、ジオメトリ処理の結果をラスタライズによりデジタル画像化して、処理するフラグメントを選択します。そして、その各フラグメントの色を決定し、フレームバッファに出力して画像を生成します (図 24)。実際のハードウェアには、これらの他にもステージがあります。

#### ラスタライズ

ラスタライズはラスタライザによって行われます。これは三角形セットアップ (Triangle Setup) と走査変換 (スキャンコンバージョン, Scan Conversion) の二つのステージで構成されます。走査変換は塗り潰し処理によって三角形内に含まれるフラグメントを選択します。三角形セットアップは三角形のディスプレイ上の頂点位置から走査変換に必要なパラメータの算出を行います。

<figure>
<img src="media/image25.emf" style="width:5in;height:3.69444in" />
<figcaption><p>図 25 ラスタライズ</p></figcaption>
</figure>

#### フラグメントの陰影付け

フラグメントの陰影付けはフラグメント処理の中心になるステージで、ラスタライザから受け取った処理対象のフラグメントに関するパラメータ (頂点位置、頂点色、頂点の法線ベクトル、テクスチャ座標などの頂点属性のそのフラグメントにおける補間値) を用いて、フラグメントの色を決定します。テクスチャマッピングもここで行います。

#### フラグメント単位の処理

フラグメントの陰影付けを行った後、その結果を最終的な画像を保持するフレームバッファに保存するまでの間にも、様々な処理が実行されます。これも以下のようなステージを持つパイプラインとして構成されています。

##### 【フォグ処理】

生成された画像に対して、大気効果 (霧やガスの効果) を与えます。遠景を描かずにごまかすときなどによく用いられます。ただし、現在これはフラグメントの陰影付けの際に行われるため、ハードウェアとして特別なステージは用意されていません。

<figure>
<img src="media/image26.emf" style="width:6in;height:3.09653in" />
<figcaption><p>図 26 フォグ処理</p></figcaption>
</figure>

##### 【ステンシルテスト】

フレームバッファを構成するバッファの一つであるステンシルバッファに格納されている値を参照して、そのフラグメントの表示の可否を決定します。ステンシルバッファの内容をマスクに用いて表示する形状の型抜きを行うことができます。

##### 【デプステスト】

フレームバッファを構成するバッファの一つであるデプスバッファに格納されている値を参照して、そのフラグメントの表示の可否を決定します。これを用いてデプスバッファ法による隠面消去処理を行うことができます。

##### 【ブレンド処理】

フレームバファの内容と表示しようとするフラグメントの値を合成します。半透明処理や画像の合成などを行うことができます。

##### 【ロジック処理】

フレームバファの内容と表示しようとするフラグメントの値をもとに論理演算を行います。

#### フレームバッファ

フレームバッファはレンダリングの最終結果を合成し、映像の表示を行うハードウェアです。これはカラーバッファ、デプスバッファ、ステンシルバッファの複数のバッファの集合体です。 フレームバッファオブジェクト (Frame Buffer Object, FBO) という機能を用いれば、 任意のバッファを組み合わせたフレームバッファをユーザが構成することができます。

##### 【カラーバッファ】

フラグメントの色を保持するバッファで、ここに画像を格納します。ダブルバッファリングを行う場合や立体視表示を行う場合は、複数のカラーバッファが用意されます。

##### 【デプスバッファ】 

デプスバッファ法による隠面消去処理に用いるバッファです。これにはフラグメントのデプス値 (深度値) が格納されます。

##### 【ステンシルバッファ】

表示形状の「型抜き」を行うために、マスクとなる画像を保持するバッファです。

##### 【アキュムレーションバッファ】

カラーバッファの内容を累積することができるバファです。これは古い機能であり、近年は使われていません。

図 27 フレームバッファの構成
